{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124 --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages for torch-2.5.1+cu124...\n",
      "Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu124.html\n",
      "Requirement already satisfied: pyg_lib in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (0.4.0+pt25cu124)\n",
      "Requirement already satisfied: torch_scatter in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (2.1.2+pt25cu124)\n",
      "Requirement already satisfied: torch_sparse in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (0.6.18+pt25cu124)\n",
      "Requirement already satisfied: torch_cluster in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (1.6.3+pt25cu124)\n",
      "Requirement already satisfied: torch_spline_conv in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (1.2.2+pt25cu124)\n",
      "Requirement already satisfied: scipy in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from torch_sparse) (1.15.2)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from scipy->torch_sparse) (2.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch_geometric in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (2.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from torch_geometric) (3.11.12)\n",
      "Requirement already satisfied: fsspec in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from torch_geometric) (2024.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from torch_geometric) (3.1.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from torch_geometric) (2.1.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from torch_geometric) (6.1.1)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from torch_geometric) (3.2.1)\n",
      "Requirement already satisfied: requests in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from torch_geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from torch_geometric) (4.67.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from aiohttp->torch_geometric) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from aiohttp->torch_geometric) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from aiohttp->torch_geometric) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from aiohttp->torch_geometric) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from aiohttp->torch_geometric) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from aiohttp->torch_geometric) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from aiohttp->torch_geometric) (1.18.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from jinja2->torch_geometric) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from requests->torch_geometric) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from requests->torch_geometric) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from requests->torch_geometric) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from requests->torch_geometric) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from tqdm->torch_geometric) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def format_pytorch_version(version_str):\n",
    "    # Example input: \"2.0.1+cu118\" -> returns \"2.0.1\"\n",
    "    return version_str.split('+')[0]\n",
    "\n",
    "def format_cuda_version(cuda_str):\n",
    "    # If CUDA is None (CPU-only PyTorch), return \"cpu\"\n",
    "    if cuda_str is None:\n",
    "        return \"cpu\"\n",
    "    # Example: \"11.8\" -> \"cu118\"\n",
    "    return \"cu\" + cuda_str.replace('.', '')\n",
    "\n",
    "TORCH_version = torch.__version__\n",
    "TORCH = format_pytorch_version(TORCH_version)\n",
    "CUDA_version = torch.version.cuda\n",
    "CUDA = format_cuda_version(CUDA_version)\n",
    "\n",
    "print(f\"Installing packages for torch-{TORCH}+{CUDA}...\")\n",
    "\n",
    "%pip install --upgrade --no-cache-dir pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
    "\n",
    "%pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: xgboost in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from scikit-learn) (2.1.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from imbalanced-learn) (0.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn imbalanced-learn xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (4.2.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from optuna) (1.14.1)\n",
      "Requirement already satisfied: colorlog in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from optuna) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from optuna) (2.0.38)\n",
      "Requirement already satisfied: tqdm in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.3.9)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\romai\\desktop\\gnn\\gnn_pu\\.conda\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization Citeseer\n",
    "#### SCAR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 11:20:21,411] A new study created in memory with name: no-name-c7765fdb-3828-49f5-bbd8-fe3279a0a332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment with seed=654:\n",
      " - K=4, layers=2, hidden=256, out=64\n",
      " - norm=None, dropout=0, batch_size=15, reliable_mini_batch=True\n",
      " - ratio=0.09767575025383361, aggregation=sum, treatment=relabeling\n",
      " - model_type=SAGEConv, rate_pairs=10, clusters=100, lr=0.00901690252087701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing METIS partitioning...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 4.0619\n",
      "Epoch 10, Loss: 3.1184\n",
      "Epoch 20, Loss: 2.0205\n",
      "Epoch 30, Loss: 1.3516\n",
      "Epoch 40, Loss: 0.9404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 11:20:52,031] Trial 0 finished with value: 0.676602086438152 and parameters: {'K': 4, 'layers': 2, 'hidden_channels': 256, 'out_channels': 64, 'ratio': 0.09767575025383361, 'aggregation': 'sum', 'treatment': 'relabeling', 'model_type': 'SAGEConv', 'rate_pairs': 10, 'batch_size': 15, 'lr': 0.00901690252087701, 'clusters': 100}. Best is trial 0 with value: 0.676602086438152.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Metrics: Accuracy=0.8696, F1=0.6766, Recall=0.6476, Precision=0.7083\n",
      "F1 = 0.68 < 0.835, skipping ...\n",
      "Done. Results written to citeseer_experimentations\\citeseer_scar_sampling_nnif_batch_cluster_1203112021.csv.\n",
      "Average F1 over valid seeds: 0.6766 ± 0.0000\n",
      "Running experiment with seed=654:\n",
      " - K=4, layers=1, hidden=64, out=64\n",
      " - norm=None, dropout=0, batch_size=1, reliable_mini_batch=True\n",
      " - ratio=0.10150501612558517, aggregation=sum, treatment=removal\n",
      " - model_type=GINConv, rate_pairs=9, clusters=100, lr=0.007912856249357784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing METIS partitioning...\n",
      "Done!\n",
      "[I 2025-03-12 11:20:55,441] Trial 1 finished with value: 0.0 and parameters: {'K': 4, 'layers': 1, 'hidden_channels': 64, 'out_channels': 64, 'ratio': 0.10150501612558517, 'aggregation': 'sum', 'treatment': 'removal', 'model_type': 'GINConv', 'rate_pairs': 9, 'batch_size': 1, 'lr': 0.007912856249357784, 'clusters': 100}. Best is trial 0 with value: 0.676602086438152.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Labels must be in {0,1}, got [0].\n",
      "Done. Results written to citeseer_experimentations\\citeseer_scar_sampling_nnif_batch_cluster_1203112052.csv.\n",
      "Average F1 over valid seeds: 0.0000 ± 0.0000\n",
      "Running experiment with seed=654:\n",
      " - K=3, layers=1, hidden=256, out=64\n",
      " - norm=None, dropout=0, batch_size=5, reliable_mini_batch=True\n",
      " - ratio=0.09189430602320858, aggregation=sum, treatment=removal\n",
      " - model_type=GATConv, rate_pairs=1, clusters=100, lr=0.004221047121711186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing METIS partitioning...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 14.4128\n",
      "Epoch 10, Loss: 8.5733\n",
      "Epoch 20, Loss: 4.9554\n",
      "Epoch 30, Loss: 3.1662\n"
     ]
    }
   ],
   "source": [
    "from train_NNIF_GNN import run_nnif_gnn_experiment\n",
    "import optuna\n",
    "from typing import Dict, Any\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Objective function for hyperparameter optimization using Optuna.\n",
    "    It builds a parameter dictionary, calls the experiment function, and\n",
    "    returns the average F1 score.\n",
    "    \"\"\"\n",
    "    params: Dict[str, Any] = {\n",
    "        \"dataset_name\": \"citeseer\",      \n",
    "        \"mechanism\": \"SCAR\",\n",
    "        \"train_pct\": 0.5,\n",
    "        \"K\": trial.suggest_int(\"K\", 2, 7),\n",
    "        \"layers\": trial.suggest_int(\"layers\", 1, 2),\n",
    "        \"hidden_channels\": trial.suggest_categorical(\"hidden_channels\", [64,128, 256]),\n",
    "        \"out_channels\": trial.suggest_categorical(\"out_channels\", [64,128, 256]),\n",
    "        \"norm\": None,\n",
    "        \"dropout\": 0,\n",
    "        \"ratio\": trial.suggest_float(\"ratio\", 0.09, 0.18),\n",
    "        \"aggregation\": trial.suggest_categorical(\"aggregation\", ['sum', 'mean']),\n",
    "        \"treatment\": trial.suggest_categorical(\"treatment\", [\"removal\", \"relabeling\"]),\n",
    "        \"model_type\": trial.suggest_categorical(\"model_type\", ['GCNConv', 'GATConv', 'SAGEConv', 'GINConv']),\n",
    "        \"rate_pairs\":trial.suggest_int(\"rate_pairs\",1,15),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [1,5,10,15]),\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-4, 1e-2),\n",
    "        \"reliable_mini_batch\":True,\n",
    "        \"seeds\": 5,\n",
    "        \"output_csv\": \"citeseer_scar_sampling_nnif_batch_cluster.csv\",\n",
    "        \"min\":0.835,\n",
    "        \"clusters\":trial.suggest_categorical(\"clusters\", [100,200,300,400,500]),\n",
    "       \n",
    "    }\n",
    "    \n",
    "    avg_f1, std_f1 = run_nnif_gnn_experiment(params)\n",
    "    \n",
    "    # We aim to maximize F1 score.\n",
    "    return avg_f1\n",
    "\n",
    "# Create an Optuna study to maximize the F1 score.\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=200)\n",
    "\n",
    "# Print out the best hyperparameters and corresponding F1 score.\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Average F1:\", trial.value)\n",
    "print(\"  Best parameters:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization Citeseer\n",
    "#### SAR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_NNIF_GNN import run_nnif_gnn_experiment\n",
    "import optuna\n",
    "from typing import Dict, Any\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Objective function for hyperparameter optimization using Optuna.\n",
    "    It builds a parameter dictionary, calls the experiment function, and\n",
    "    returns the average F1 score.\n",
    "    \"\"\"\n",
    "    params: Dict[str, Any] = {\n",
    "        \"dataset_name\": \"citeseer\",      \n",
    "        \"mechanism\": \"SAR\",\n",
    "        \"train_pct\": 0.5,\n",
    "        \"K\": trial.suggest_int(\"K\", 2, 7),\n",
    "        \"layers\": trial.suggest_int(\"layers\", 1, 2),\n",
    "        \"hidden_channels\": trial.suggest_categorical(\"hidden_channels\", [64,128, 256]),\n",
    "        \"out_channels\": trial.suggest_categorical(\"out_channels\", [64,128, 256]),\n",
    "        \"norm\": None,\n",
    "        \"dropout\": 0,\n",
    "        \"ratio\": trial.suggest_float(\"ratio\", 0.09, 0.18),\n",
    "        \"aggregation\": trial.suggest_categorical(\"aggregation\", ['sum', 'mean']),\n",
    "        \"treatment\": trial.suggest_categorical(\"treatment\", [\"removal\", \"relabeling\"]),\n",
    "        \"model_type\": trial.suggest_categorical(\"model_type\", ['GCNConv', 'GATConv', 'SAGEConv', 'GINConv']),\n",
    "        \"rate_pairs\":trial.suggest_int(\"rate_pairs\",1,10),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [128,256,512,1024,2048]),\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-4, 1e-2),\n",
    "        \"reliable_mini_batch\":True,\n",
    "        \"seeds\": 5,\n",
    "        \"output_csv\": \"citeseer_sar_sampling_nnif_batch_cluster.csv\",\n",
    "        \"min\":0.82\n",
    "    }\n",
    "    \n",
    "    avg_f1, std_f1 = run_nnif_gnn_experiment(params)\n",
    "    \n",
    "    # We aim to maximize F1 score.\n",
    "    return avg_f1\n",
    "\n",
    "# Create an Optuna study to maximize the F1 score.\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=200)\n",
    "\n",
    "# Print out the best hyperparameters and corresponding F1 score.\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Average F1:\", trial.value)\n",
    "print(\"  Best parameters:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization Cora\n",
    "#### SCAR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_NNIF_GNN import run_nnif_gnn_experiment\n",
    "import optuna\n",
    "from typing import Dict, Any\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Objective function for hyperparameter optimization using Optuna.\n",
    "    It builds a parameter dictionary, calls the experiment function, and\n",
    "    returns the average F1 score.\n",
    "    \"\"\"\n",
    "    params: Dict[str, Any] = {\n",
    "        \"dataset_name\": \"cora\",      \n",
    "        \"mechanism\": \"SCAR\",\n",
    "        \"train_pct\": 0.5,\n",
    "        \"K\": trial.suggest_int(\"K\", 4, 12),\n",
    "        \"layers\": trial.suggest_int(\"layers\", 1, 2),\n",
    "        \"hidden_channels\": trial.suggest_categorical(\"hidden_channels\", [64,128, 256]),\n",
    "        \"out_channels\": trial.suggest_categorical(\"out_channels\", [64,128, 256]),\n",
    "        \"norm\": None,\n",
    "        \"dropout\": 0,\n",
    "        \"ratio\": trial.suggest_float(\"ratio\", 0.16, 0.34),\n",
    "        \"aggregation\": trial.suggest_categorical(\"aggregation\", ['sum', 'mean']),\n",
    "        \"treatment\": trial.suggest_categorical(\"treatment\", [\"removal\", \"relabeling\"]),\n",
    "        \"model_type\": trial.suggest_categorical(\"model_type\", ['GCNConv', 'GATConv', 'SAGEConv', 'GINConv']),\n",
    "        \"rate_pairs\":trial.suggest_int(\"rate_pairs\",1,15),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [1,5,10,15]),\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-4, 1e-2),\n",
    "        \"reliable_mini_batch\":True,\n",
    "        \"seeds\": 5,\n",
    "        \"output_csv\": \"cora_scar_sampling_nnif_batch_cluster.csv\",\n",
    "        \"min\":0.84,\n",
    "        \"clusters\":trial.suggest_categorical(\"clusters\", [100,200,300,400,500]),\n",
    "       \n",
    "    }\n",
    "    \n",
    "    avg_f1, std_f1 = run_nnif_gnn_experiment(params)\n",
    "    \n",
    "    # We aim to maximize F1 score.\n",
    "    return avg_f1\n",
    "\n",
    "# Create an Optuna study to maximize the F1 score.\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=200)\n",
    "\n",
    "# Print out the best hyperparameters and corresponding F1 score.\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Average F1:\", trial.value)\n",
    "print(\"  Best parameters:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization Cora\n",
    "#### SAR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_NNIF_GNN import run_nnif_gnn_experiment\n",
    "import optuna\n",
    "from typing import Dict, Any\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Objective function for hyperparameter optimization using Optuna.\n",
    "    It builds a parameter dictionary, calls the experiment function, and\n",
    "    returns the average F1 score.\n",
    "    \"\"\"\n",
    "    params: Dict[str, Any] = {\n",
    "        \"dataset_name\": \"cora\",      \n",
    "        \"mechanism\": \"SAR\",\n",
    "        \"train_pct\": 0.5,\n",
    "        \"K\": trial.suggest_int(\"K\", 4, 12),\n",
    "        \"layers\": trial.suggest_int(\"layers\", 1, 2),\n",
    "        \"hidden_channels\": trial.suggest_categorical(\"hidden_channels\", [64,128, 256]),\n",
    "        \"out_channels\": trial.suggest_categorical(\"out_channels\", [64,128, 256]),\n",
    "        \"norm\": None,\n",
    "        \"dropout\": 0,\n",
    "        \"ratio\": trial.suggest_float(\"ratio\", 0.16, 0.34),\n",
    "        \"aggregation\": trial.suggest_categorical(\"aggregation\", ['sum', 'mean']),\n",
    "        \"treatment\": trial.suggest_categorical(\"treatment\", [\"removal\", \"relabeling\"]),\n",
    "        \"model_type\": trial.suggest_categorical(\"model_type\", ['GCNConv', 'GATConv', 'SAGEConv', 'GINConv']),\n",
    "        \"rate_pairs\":trial.suggest_int(\"rate_pairs\",1,10),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [128,256,512,1024,2048]),\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-4, 1e-2),\n",
    "        \"reliable_mini_batch\":True,\n",
    "        \"seeds\": 5,\n",
    "        \"output_csv\": \"cora_sar_sampling_nnif_batch_cluster.csv\",\n",
    "        \"min\":0.84\n",
    "    }\n",
    "    \n",
    "    avg_f1, std_f1 = run_nnif_gnn_experiment(params)\n",
    "    \n",
    "    # We aim to maximize F1 score.\n",
    "    return avg_f1\n",
    "\n",
    "# Create an Optuna study to maximize the F1 score.\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=200)\n",
    "\n",
    "# Print out the best hyperparameters and corresponding F1 score.\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Average F1:\", trial.value)\n",
    "print(\"  Best parameters:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization Pubmed\n",
    "#### SCAR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_NNIF_GNN import run_nnif_gnn_experiment\n",
    "import optuna\n",
    "from typing import Dict, Any\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Objective function for hyperparameter optimization using Optuna.\n",
    "    It builds a parameter dictionary, calls the experiment function, and\n",
    "    returns the average F1 score.\n",
    "    \"\"\"\n",
    "    params: Dict[str, Any] = {\n",
    "        \"dataset_name\": \"pubmed\",      \n",
    "        \"mechanism\": \"SCAR\",\n",
    "        \"train_pct\": 0.5,\n",
    "        \"K\": trial.suggest_int(\"K\", 5, 14),\n",
    "        \"layers\": trial.suggest_int(\"layers\", 1, 2),\n",
    "        \"hidden_channels\": trial.suggest_categorical(\"hidden_channels\", [64,128, 256]),\n",
    "        \"out_channels\": trial.suggest_categorical(\"out_channels\", [64,128, 256]),\n",
    "        \"norm\": None,\n",
    "        \"dropout\": 0,\n",
    "        \"ratio\": trial.suggest_float(\"ratio\", 0.2, 0.4),\n",
    "        \"aggregation\": trial.suggest_categorical(\"aggregation\", ['sum', 'mean']),\n",
    "        \"treatment\": trial.suggest_categorical(\"treatment\", [\"removal\", \"relabeling\"]),\n",
    "        \"model_type\": trial.suggest_categorical(\"model_type\", ['GCNConv', 'GATConv', 'SAGEConv', 'GINConv']),\n",
    "        \"rate_pairs\":trial.suggest_int(\"rate_pairs\",1,15),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [1,5,10,15]),\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-4, 1e-2),\n",
    "        \"reliable_mini_batch\":True,\n",
    "        \"seeds\": 5,\n",
    "        \"output_csv\": \"pubmed_scar_sampling_nnif_batch_cluster.csv\",\n",
    "        \"min\":0.84,\n",
    "        \"clusters\":trial.suggest_categorical(\"clusters\", [100,200,300,400,500]),\n",
    "       \n",
    "    }\n",
    "    \n",
    "    avg_f1, std_f1 = run_nnif_gnn_experiment(params)\n",
    "    \n",
    "    # We aim to maximize F1 score.\n",
    "    return avg_f1\n",
    "\n",
    "# Create an Optuna study to maximize the F1 score.\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=200)\n",
    "\n",
    "# Print out the best hyperparameters and corresponding F1 score.\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Average F1:\", trial.value)\n",
    "print(\"  Best parameters:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization Pubmed\n",
    "#### SAR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_NNIF_GNN import run_nnif_gnn_experiment\n",
    "import optuna\n",
    "from typing import Dict, Any\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Objective function for hyperparameter optimization using Optuna.\n",
    "    It builds a parameter dictionary, calls the experiment function, and\n",
    "    returns the average F1 score.\n",
    "    \"\"\"\n",
    "    params: Dict[str, Any] = {\n",
    "        \"dataset_name\": \"pubmed\",      \n",
    "        \"mechanism\": \"SAR\",\n",
    "        \"train_pct\": 0.5,\n",
    "        \"K\": trial.suggest_int(\"K\", 6, 14),\n",
    "        \"layers\": trial.suggest_int(\"layers\", 1, 2),\n",
    "        \"hidden_channels\": trial.suggest_categorical(\"hidden_channels\", [64, 128, 256]),\n",
    "        \"out_channels\": trial.suggest_categorical(\"out_channels\", [64, 128, 256]),\n",
    "        \"norm\": None,\n",
    "        \"dropout\": 0,\n",
    "        \"ratio\": trial.suggest_float(\"ratio\", 0.23, 0.27),\n",
    "        \"aggregation\": trial.suggest_categorical(\"aggregation\", ['sum', 'mean']),\n",
    "        \"treatment\": trial.suggest_categorical(\"treatment\", [\"removal\", \"relabeling\"]),\n",
    "        \"model_type\": trial.suggest_categorical(\"model_type\", ['GCNConv', 'GATConv', 'SAGEConv', 'GINConv']),\n",
    "        \"rate_pairs\":trial.suggest_int(\"rate_pairs\",1,10),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [512,1024,2048,5096,10192]),\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-5, 1e-2),\n",
    "        \"reliable_mini_batch\":True,\n",
    "        \"seeds\": 5,\n",
    "        \"output_csv\": \"pubmed_sar_sampling_nnif_batch_cluster.csv\",\n",
    "        \"min\": 0.82\n",
    "        }\n",
    "    \n",
    "    # Call the experiment function with these parameters.\n",
    "    # run_nnif_gnn_experiment returns (avg_f1, std_f1)\n",
    "    avg_f1, std_f1 = run_nnif_gnn_experiment(params)\n",
    "    \n",
    "    # We aim to maximize F1 score.\n",
    "    return avg_f1\n",
    "\n",
    "# Create an Optuna study to maximize the F1 score.\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=200)\n",
    "\n",
    "# Print out the best hyperparameters and corresponding F1 score.\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Average F1:\", trial.value)\n",
    "print(\"  Best parameters:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization WikiCS\n",
    "#### SCAR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_NNIF_GNN import run_nnif_gnn_experiment\n",
    "import optuna\n",
    "from typing import Dict, Any\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Objective function for hyperparameter optimization using Optuna.\n",
    "    It builds a parameter dictionary, calls the experiment function, and\n",
    "    returns the average F1 score.\n",
    "    \"\"\"\n",
    "    params: Dict[str, Any] = {\n",
    "        \"dataset_name\": \"wiki-cs\",      \n",
    "        \"mechanism\": \"SCAR\",\n",
    "        \"train_pct\": 0.5,\n",
    "        \"K\": trial.suggest_int(\"K\", 3, 40),\n",
    "        \"layers\": trial.suggest_int(\"layers\", 1, 2),\n",
    "        \"hidden_channels\": trial.suggest_categorical(\"hidden_channels\", [64,128, 256]),\n",
    "        \"out_channels\": trial.suggest_categorical(\"out_channels\", [64,128, 256]),\n",
    "        \"norm\": None,\n",
    "        \"dropout\": 0,\n",
    "        \"ratio\": trial.suggest_float(\"ratio\", 0.09, 0.18),\n",
    "        \"aggregation\": trial.suggest_categorical(\"aggregation\", ['sum', 'mean']),\n",
    "        \"treatment\": trial.suggest_categorical(\"treatment\", [\"removal\", \"relabeling\"]),\n",
    "        \"model_type\": trial.suggest_categorical(\"model_type\", ['GCNConv', 'GATConv', 'SAGEConv', 'GINConv']),\n",
    "        \"rate_pairs\":trial.suggest_int(\"rate_pairs\",1,10),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [1,5,10,15]),\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-4, 1e-2),\n",
    "        \"reliable_mini_batch\":True,\n",
    "        \"seeds\": 5,\n",
    "        \"output_csv\": \"wikics_scar_sampling_nnif_batch_cluster.csv\",\n",
    "        \"min\":0.88,\n",
    "        \"clusters\":trial.suggest_categorical(\"clusters\", [100,200,300,400,500]),\n",
    "       \n",
    "    }\n",
    "    \n",
    "    avg_f1, std_f1 = run_nnif_gnn_experiment(params)\n",
    "    \n",
    "    # We aim to maximize F1 score.\n",
    "    return avg_f1\n",
    "\n",
    "# Create an Optuna study to maximize the F1 score.\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=200)\n",
    "\n",
    "# Print out the best hyperparameters and corresponding F1 score.\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Average F1:\", trial.value)\n",
    "print(\"  Best parameters:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization WikiCS\n",
    "#### SAR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_NNIF_GNN import run_nnif_gnn_experiment\n",
    "import optuna\n",
    "from typing import Dict, Any\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Objective function for hyperparameter optimization using Optuna.\n",
    "    It builds a parameter dictionary, calls the experiment function, and\n",
    "    returns the average F1 score.\n",
    "    \"\"\"\n",
    "    params: Dict[str, Any] = {\n",
    "        \"dataset_name\": \"wiki-cs\",      \n",
    "        \"mechanism\": \"SAR\",\n",
    "        \"train_pct\": 0.5,\n",
    "        \"alpha\": 0.5,\n",
    "        \"K\": trial.suggest_int(\"K\", 23, 48),\n",
    "        \"layers\": trial.suggest_int(\"layers\", 1, 2),\n",
    "        \"hidden_channels\": trial.suggest_categorical(\"hidden_channels\", [64,128, 256]),\n",
    "        \"out_channels\": trial.suggest_categorical(\"out_channels\", [64,128, 256]),\n",
    "        \"norm\": None,\n",
    "        \"dropout\": 0,#trial.suggest_float(\"dropout\", 0, 0.5),\n",
    "        \"margin\": 0.5,\n",
    "        \"lpl_weight\": 0.5,#trial.suggest_float(\"lpl_weight\", 0.01, 0.99),\n",
    "        \"ratio\": trial.suggest_float(\"ratio\", 0.08, 0.48),\n",
    "        \"aggregation\": trial.suggest_categorical(\"aggregation\", ['sum', 'mean']),\n",
    "        \"treatment\": trial.suggest_categorical(\"treatment\", [\"removal\", \"relabeling\"]),\n",
    "        \"model_type\": trial.suggest_categorical(\"model_type\", ['GCNConv', 'GATConv', 'SAGEConv', 'GINConv', 'TransformerConv']),\n",
    "        \"sampling_mode\":trial.suggest_categorical(\"sampling_mode\", [None,'nn', 'random', 'feature']), #'weighted',\n",
    "        \"num_neighbors\": trial.suggest_int(\"num_neighbors\", 5, 20),\n",
    "        \"seeds\": 5,\n",
    "        \"output_csv\": \"wikics_sar_conv_sampling.csv\"\n",
    "    }\n",
    "    \n",
    "    # Call the experiment function with these parameters.\n",
    "    # run_nnif_gnn_experiment returns (avg_f1, std_f1)\n",
    "    avg_f1, std_f1 = run_nnif_gnn_experiment(params)\n",
    "    \n",
    "    # We aim to maximize F1 score.\n",
    "    return avg_f1\n",
    "\n",
    "# Create an Optuna study to maximize the F1 score.\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=200)\n",
    "\n",
    "# Print out the best hyperparameters and corresponding F1 score.\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Average F1:\", trial.value)\n",
    "print(\"  Best parameters:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elliptic Bitcoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 11:08:16,457] A new study created in memory with name: no-name-9f843de6-1c81-4682-859c-d54c36ea9fdb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment with seed=654:\n",
      " - K=16, layers=2, hidden=256, out=128\n",
      " - norm=None, dropout=0.38706779927156976, batch_size=40, reliable_mini_batch=True\n",
      " - ratio=0.01222827988140382, aggregation=sum, treatment=removal\n",
      " - model_type=SAGEConv, rate_pairs=3, clusters=2000, lr=0.005651409078608822\n",
      "165\n",
      "torch.Size([203769, 165])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing METIS partitioning...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 674.1356\n",
      "Epoch 10, Loss: 127.3564\n",
      "Epoch 20, Loss: 50.7907\n",
      "Epoch 30, Loss: 26.9527\n",
      "Epoch 40, Loss: 16.4045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 11:19:05,538] Trial 0 finished with value: 0.6324221233312143 and parameters: {'K': 16, 'layers': 2, 'hidden_channels': 256, 'out_channels': 128, 'dropout': 0.38706779927156976, 'ratio': 0.01222827988140382, 'aggregation': 'sum', 'treatment': 'removal', 'model_type': 'SAGEConv', 'rate_pairs': 3, 'batch_size': 40, 'lr': 0.005651409078608822, 'clusters': 2000}. Best is trial 0 with value: 0.6324221233312143.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Metrics: Accuracy=0.9379, F1=0.6324, Recall=0.5472, Precision=0.7491\n",
      "F1 = 0.63 < 0.65, skipping ...\n",
      "Done. Results written to elliptic-bitcoin_experimentations\\elliptic_bitcoin_cluster_1203110816.csv.\n",
      "Average F1 over valid seeds: 0.6324 ± 0.0000\n",
      "Running experiment with seed=654:\n",
      " - K=8, layers=1, hidden=64, out=64\n",
      " - norm=None, dropout=0.16686621113334402, batch_size=40, reliable_mini_batch=True\n",
      " - ratio=0.01494468463150106, aggregation=sum, treatment=relabeling\n",
      " - model_type=SAGEConv, rate_pairs=1, clusters=2000, lr=0.000584308231031967\n",
      "165\n",
      "torch.Size([203769, 165])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing METIS partitioning...\n"
     ]
    }
   ],
   "source": [
    "from train_NNIF_GNN import run_nnif_gnn_experiment\n",
    "import optuna\n",
    "from typing import Dict, Any\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Objective function for hyperparameter optimization using Optuna.\n",
    "    It builds a parameter dictionary, calls the experiment function, and\n",
    "    returns the average F1 score.\n",
    "    \"\"\"\n",
    "    params: Dict[str, Any] = {\n",
    "        \"dataset_name\": \"elliptic-bitcoin\",      \n",
    "        \"mechanism\": \"SAR2\",\n",
    "        \"train_pct\": 0.5,\n",
    "        \"alpha\": 0.5,\n",
    "        \"K\": trial.suggest_int(\"K\", 3, 20),\n",
    "        \"layers\": trial.suggest_int(\"layers\", 1, 3),\n",
    "        \"hidden_channels\": trial.suggest_categorical(\"hidden_channels\", [64,128, 256]),\n",
    "        \"out_channels\": trial.suggest_categorical(\"out_channels\", [64,128, 256]),\n",
    "        \"norm\": None,\n",
    "        \"dropout\": trial.suggest_float(\"dropout\", 0, 0.4),\n",
    "        \"ratio\": trial.suggest_float(\"ratio\", 0.001, 0.03),\n",
    "        \"aggregation\": trial.suggest_categorical(\"aggregation\", ['sum', 'mean']),\n",
    "        \"treatment\": trial.suggest_categorical(\"treatment\", [\"removal\", \"relabeling\"]),\n",
    "        \"model_type\": trial.suggest_categorical(\"model_type\", ['GCNConv', 'GATConv', 'SAGEConv', 'GINConv']),\n",
    "        \"rate_pairs\":trial.suggest_int(\"rate_pairs\",1,10),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [10,20,30,40]),\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-4, 1e-2),\n",
    "        \"reliable_mini_batch\":True,\n",
    "        \"seeds\": 1,\n",
    "        \"output_csv\": \"elliptic_bitcoin_cluster.csv\",\n",
    "        \"min\":0.65,\n",
    "        \"clusters\":trial.suggest_categorical(\"clusters\", [1300,1500,1700,2000])\n",
    "        }\n",
    "\n",
    "    \n",
    "    # Call the experiment function with these parameters.\n",
    "    # run_nnif_gnn_experiment returns (avg_f1, std_f1)\n",
    "    avg_f1, std_f1 = run_nnif_gnn_experiment(params)\n",
    "    \n",
    "    # We aim to maximize F1 score.\n",
    "    return avg_f1\n",
    "\n",
    "# Create an Optuna study to maximize the F1 score.\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=200)\n",
    "\n",
    "# Print out the best hyperparameters and corresponding F1 score.\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Average F1:\", trial.value)\n",
    "print(\"  Best parameters:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
