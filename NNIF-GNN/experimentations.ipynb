{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch==2.5.1+cu124 (from versions: 2.6.0)\n",
      "ERROR: No matching distribution found for torch==2.5.1+cu124\n"
     ]
    }
   ],
   "source": [
    "%pip install torch==2.5.1+cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages for torch-2.6.0+cu126...\n",
      "Looking in links: https://data.pyg.org/whl/torch-2.6.0+cu126.html\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement pyg_lib (from versions: none)\n",
      "ERROR: No matching distribution found for pyg_lib\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch_geometric in c:\\users\\romai\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\romai\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch_geometric) (3.11.12)\n",
      "Requirement already satisfied: fsspec in c:\\users\\romai\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch_geometric) (2025.2.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\romai\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch_geometric) (3.1.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\romai\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch_geometric) (2.1.3)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\romai\\appdata\\roaming\\python\\python313\\site-packages (from torch_geometric) (7.0.0)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\romai\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch_geometric) (3.2.1)\n",
      "Requirement already satisfied: requests in c:\\users\\romai\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch_geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\romai\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch_geometric) (4.67.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\romai\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->torch_geometric) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\romai\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->torch_geometric) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\romai\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->torch_geometric) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\romai\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->torch_geometric) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\romai\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->torch_geometric) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\romai\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->torch_geometric) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\romai\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp->torch_geometric) (1.18.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\romai\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch_geometric) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\romai\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->torch_geometric) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\romai\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->torch_geometric) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\romai\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->torch_geometric) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\romai\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->torch_geometric) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\romai\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->torch_geometric) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def format_pytorch_version(version_str):\n",
    "    # Example input: \"2.0.1+cu118\" -> returns \"2.0.1\"\n",
    "    return version_str.split('+')[0]\n",
    "\n",
    "def format_cuda_version(cuda_str):\n",
    "    # If CUDA is None (CPU-only PyTorch), return \"cpu\"\n",
    "    if cuda_str is None:\n",
    "        return \"cpu\"\n",
    "    # Example: \"11.8\" -> \"cu118\"\n",
    "    return \"cu\" + cuda_str.replace('.', '')\n",
    "\n",
    "TORCH_version = torch.__version__\n",
    "TORCH = format_pytorch_version(TORCH_version)\n",
    "CUDA_version = torch.version.cuda\n",
    "CUDA = format_cuda_version(CUDA_version)\n",
    "\n",
    "print(f\"Installing packages for torch-{TORCH}+{CUDA}...\")\n",
    "\n",
    "%pip install --upgrade --no-cache-dir pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
    "\n",
    "%pip install torch_geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization Citeseer\n",
    "#### SCAR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\romai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtrain_NNIF_GNN\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_nnif_gnn_experiment\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moptuna\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Any\n",
      "File \u001b[1;32mc:\\Users\\romai\\Desktop\\gnn\\gnn_pu\\NNIF-GNN\\train_NNIF_GNN.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m add_self_loops, coalesce\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Data\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch_sparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparseTensor\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Tuple, List, Any\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch_sparse'"
     ]
    }
   ],
   "source": [
    "from train_NNIF_GNN import run_nnif_gnn_experiment\n",
    "import optuna\n",
    "from typing import Dict, Any\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Objective function for hyperparameter optimization using Optuna.\n",
    "    It builds a parameter dictionary, calls the experiment function, and\n",
    "    returns the average F1 score.\n",
    "    \"\"\"\n",
    "    params: Dict[str, Any] = {\n",
    "        \"dataset_name\": \"citeseer\",      \n",
    "        \"mechanism\": \"SCAR\",\n",
    "        \"train_pct\": 0.5,\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0.1, 1.0),\n",
    "        \"K\": trial.suggest_int(\"K\", 17, 23),\n",
    "        \"layers\": trial.suggest_int(\"layers\", 1, 2),\n",
    "        \"hidden_channels\": trial.suggest_categorical(\"hidden_channels\", [64, 128, 256]),\n",
    "        \"out_channels\": trial.suggest_categorical(\"out_channels\", [64, 128, 256]),\n",
    "        \"norm\": trial.suggest_categorical(\"norm\", [None, \"layernorm\", \"graphnorm\"]),\n",
    "        \"dropout\": trial.suggest_float(\"dropout\", 0.1, 0.5),\n",
    "        \"margin\": trial.suggest_float(\"margin\", 0.2, 0.8),\n",
    "        \"lpl_weight\": trial.suggest_float(\"lpl_weight\", 0.1, 0.9),\n",
    "        \"ratio\": trial.suggest_float(\"ratio\", 0.1, 0.5),\n",
    "        \"pos_weight\": 1,\n",
    "        \"aggregation\": trial.suggest_categorical(\"aggregation\", [\"sum\", \"mean\", \"max\"]),\n",
    "        \"treatment\": trial.suggest_categorical(\"treatment\", [\"removal\", \"relabeling\"]),\n",
    "        \"seeds\": 5,\n",
    "        \"output_csv\": \"citeseer_scar.csv\"\n",
    "    }\n",
    "    \n",
    "    # Call the experiment function with these parameters.\n",
    "    # run_nnif_gnn_experiment returns (avg_f1, std_f1)\n",
    "    avg_f1, std_f1 = run_nnif_gnn_experiment(params)\n",
    "    \n",
    "    # We aim to maximize F1 score.\n",
    "    return avg_f1\n",
    "\n",
    "# Create an Optuna study to maximize the F1 score.\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print out the best hyperparameters and corresponding F1 score.\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Average F1:\", trial.value)\n",
    "print(\"  Best parameters:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization Citeseer\n",
    "#### SAR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .train_NNIF_GNN import run_nnif_gnn_experiment\n",
    "import optuna\n",
    "from typing import Dict, Any\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Objective function for hyperparameter optimization using Optuna.\n",
    "    It builds a parameter dictionary, calls the experiment function, and\n",
    "    returns the average F1 score.\n",
    "    \"\"\"\n",
    "    params: Dict[str, Any] = {\n",
    "        \"dataset_name\": \"citeseer\",      \n",
    "        \"mechanism\": \"SAR\",\n",
    "        \"train_pct\": 0.5,\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0.1, 1.0),\n",
    "        \"K\": trial.suggest_int(\"K\", 15, 25),\n",
    "        \"layers\": trial.suggest_int(\"layers\", 1, 3),\n",
    "        \"hidden_channels\": trial.suggest_categorical(\"hidden_channels\", [64, 128, 256]),\n",
    "        \"out_channels\": trial.suggest_categorical(\"out_channels\", [64, 128, 256]),\n",
    "        \"norm\": trial.suggest_categorical(\"norm\", [None, \"layernorm\", \"graphnorm\"]),\n",
    "        \"dropout\": trial.suggest_float(\"dropout\", 0.1, 0.5),\n",
    "        \"margin\": trial.suggest_float(\"margin\", 0.1, 1.0),\n",
    "        \"lpl_weight\": trial.suggest_float(\"lpl_weight\", 0.1, 1.0),\n",
    "        \"ratio\": trial.suggest_float(\"ratio\", 0.1, 0.5),\n",
    "        \"pos_weight\": trial.suggest_float(\"pos_weight\", 0.5, 2.0),\n",
    "        \"aggregation\": trial.suggest_categorical(\"aggregation\", [\"sum\", \"mean\", \"max\"]),\n",
    "        \"treatment\": trial.suggest_categorical(\"treatment\", [\"removal\", \"relabeling\"]),\n",
    "        \"seeds\": 5,\n",
    "        \"output_csv\": \"citeseer_sar.csv\"\n",
    "    }\n",
    "    \n",
    "    # Call the experiment function with these parameters.\n",
    "    # run_nnif_gnn_experiment returns (avg_f1, std_f1)\n",
    "    avg_f1, std_f1 = run_nnif_gnn_experiment(params)\n",
    "    \n",
    "    # We aim to maximize F1 score.\n",
    "    return avg_f1\n",
    "\n",
    "# Create an Optuna study to maximize the F1 score.\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print out the best hyperparameters and corresponding F1 score.\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Average F1:\", trial.value)\n",
    "print(\"  Best parameters:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization Cora\n",
    "#### SCAR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .train_NNIF_GNN import run_nnif_gnn_experiment\n",
    "import optuna\n",
    "from typing import Dict, Any\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Objective function for hyperparameter optimization using Optuna.\n",
    "    It builds a parameter dictionary, calls the experiment function, and\n",
    "    returns the average F1 score.\n",
    "    \"\"\"\n",
    "    params: Dict[str, Any] = {\n",
    "        \"dataset_name\": \"cora\",      \n",
    "        \"mechanism\": \"SAR\",\n",
    "        \"train_pct\": 0.5,\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0.1, 1.0),\n",
    "        \"K\": trial.suggest_int(\"K\", 25, 35),\n",
    "        \"layers\": trial.suggest_int(\"layers\", 1, 3),\n",
    "        \"hidden_channels\": trial.suggest_categorical(\"hidden_channels\", [64, 128, 256]),\n",
    "        \"out_channels\": trial.suggest_categorical(\"out_channels\", [64, 128, 256]),\n",
    "        \"norm\": trial.suggest_categorical(\"norm\", [None, \"layernorm\", \"graphnorm\"]),\n",
    "        \"dropout\": trial.suggest_float(\"dropout\", 0.1, 0.5),\n",
    "        \"margin\": trial.suggest_float(\"margin\", 0.1, 1.0),\n",
    "        \"lpl_weight\": trial.suggest_float(\"lpl_weight\", 0.1, 1.0),\n",
    "        \"ratio\": trial.suggest_float(\"ratio\", 0.1, 0.5),\n",
    "        \"pos_weight\": trial.suggest_float(\"pos_weight\", 0.5, 2.0),\n",
    "        \"aggregation\": trial.suggest_categorical(\"aggregation\", [\"sum\", \"mean\", \"max\"]),\n",
    "        \"treatment\": trial.suggest_categorical(\"treatment\", [\"removal\", \"relabeling\"]),\n",
    "        \"seeds\": 5,\n",
    "        \"output_csv\": \"cora_scar.csv\"\n",
    "    }\n",
    "    \n",
    "    # Call the experiment function with these parameters.\n",
    "    # run_nnif_gnn_experiment returns (avg_f1, std_f1)\n",
    "    avg_f1, std_f1 = run_nnif_gnn_experiment(params)\n",
    "    \n",
    "    # We aim to maximize F1 score.\n",
    "    return avg_f1\n",
    "\n",
    "# Create an Optuna study to maximize the F1 score.\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print out the best hyperparameters and corresponding F1 score.\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Average F1:\", trial.value)\n",
    "print(\"  Best parameters:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization Cora\n",
    "#### SAR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .train_NNIF_GNN import run_nnif_gnn_experiment\n",
    "import optuna\n",
    "from typing import Dict, Any\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Objective function for hyperparameter optimization using Optuna.\n",
    "    It builds a parameter dictionary, calls the experiment function, and\n",
    "    returns the average F1 score.\n",
    "    \"\"\"\n",
    "    params: Dict[str, Any] = {\n",
    "        \"dataset_name\": \"cora\",      \n",
    "        \"mechanism\": \"SAR\",\n",
    "        \"train_pct\": 0.5,\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0.1, 1.0),\n",
    "        \"K\": trial.suggest_int(\"K\", 25, 35),\n",
    "        \"layers\": trial.suggest_int(\"layers\", 1, 3),\n",
    "        \"hidden_channels\": trial.suggest_categorical(\"hidden_channels\", [64, 128, 256]),\n",
    "        \"out_channels\": trial.suggest_categorical(\"out_channels\", [64, 128, 256]),\n",
    "        \"norm\": trial.suggest_categorical(\"norm\", [None, \"layernorm\", \"graphnorm\"]),\n",
    "        \"dropout\": trial.suggest_float(\"dropout\", 0.1, 0.5),\n",
    "        \"margin\": trial.suggest_float(\"margin\", 0.1, 1.0),\n",
    "        \"lpl_weight\": trial.suggest_float(\"lpl_weight\", 0.1, 1.0),\n",
    "        \"ratio\": trial.suggest_float(\"ratio\", 0.1, 0.5),\n",
    "        \"pos_weight\": trial.suggest_float(\"pos_weight\", 0.5, 2.0),\n",
    "        \"aggregation\": trial.suggest_categorical(\"aggregation\", [\"sum\", \"mean\", \"max\"]),\n",
    "        \"treatment\": trial.suggest_categorical(\"treatment\", [\"removal\", \"relabeling\"]),\n",
    "        \"seeds\": 5,\n",
    "        \"output_csv\": \"cora_sar.csv\"\n",
    "    }\n",
    "    \n",
    "    # Call the experiment function with these parameters.\n",
    "    # run_nnif_gnn_experiment returns (avg_f1, std_f1)\n",
    "    avg_f1, std_f1 = run_nnif_gnn_experiment(params)\n",
    "    \n",
    "    # We aim to maximize F1 score.\n",
    "    return avg_f1\n",
    "\n",
    "# Create an Optuna study to maximize the F1 score.\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print out the best hyperparameters and corresponding F1 score.\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Average F1:\", trial.value)\n",
    "print(\"  Best parameters:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization Pubmed\n",
    "#### SCAR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .train_NNIF_GNN import run_nnif_gnn_experiment\n",
    "import optuna\n",
    "from typing import Dict, Any\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Objective function for hyperparameter optimization using Optuna.\n",
    "    It builds a parameter dictionary, calls the experiment function, and\n",
    "    returns the average F1 score.\n",
    "    \"\"\"\n",
    "    params: Dict[str, Any] = {\n",
    "        \"dataset_name\": \"pubmed\",      \n",
    "        \"mechanism\": \"SCAR\",\n",
    "        \"train_pct\": 0.5,\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0.1, 1.0),\n",
    "        \"K\": trial.suggest_int(\"K\", 25, 35),\n",
    "        \"layers\": trial.suggest_int(\"layers\", 1, 3),\n",
    "        \"hidden_channels\": trial.suggest_categorical(\"hidden_channels\", [64, 128, 256]),\n",
    "        \"out_channels\": trial.suggest_categorical(\"out_channels\", [64, 128, 256]),\n",
    "        \"norm\": trial.suggest_categorical(\"norm\", [None, \"layernorm\", \"graphnorm\"]),\n",
    "        \"dropout\": trial.suggest_float(\"dropout\", 0.1, 0.5),\n",
    "        \"margin\": trial.suggest_float(\"margin\", 0.1, 1.0),\n",
    "        \"lpl_weight\": trial.suggest_float(\"lpl_weight\", 0.1, 1.0),\n",
    "        \"ratio\": trial.suggest_float(\"ratio\", 0.1, 0.5),\n",
    "        \"pos_weight\": trial.suggest_float(\"pos_weight\", 0.5, 2.0),\n",
    "        \"aggregation\": trial.suggest_categorical(\"aggregation\", [\"sum\", \"mean\", \"max\"]),\n",
    "        \"treatment\": trial.suggest_categorical(\"treatment\", [\"removal\", \"relabeling\"]),\n",
    "        \"seeds\": 5,\n",
    "        \"output_csv\": \"pubmed_scar.csv\"\n",
    "    }\n",
    "    \n",
    "    # Call the experiment function with these parameters.\n",
    "    # run_nnif_gnn_experiment returns (avg_f1, std_f1)\n",
    "    avg_f1, std_f1 = run_nnif_gnn_experiment(params)\n",
    "    \n",
    "    # We aim to maximize F1 score.\n",
    "    return avg_f1\n",
    "\n",
    "# Create an Optuna study to maximize the F1 score.\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print out the best hyperparameters and corresponding F1 score.\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Average F1:\", trial.value)\n",
    "print(\"  Best parameters:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization Pubmed\n",
    "#### SAR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .train_NNIF_GNN import run_nnif_gnn_experiment\n",
    "import optuna\n",
    "from typing import Dict, Any\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Objective function for hyperparameter optimization using Optuna.\n",
    "    It builds a parameter dictionary, calls the experiment function, and\n",
    "    returns the average F1 score.\n",
    "    \"\"\"\n",
    "    params: Dict[str, Any] = {\n",
    "        \"dataset_name\": \"pubmed\",      \n",
    "        \"mechanism\": \"SAR\",\n",
    "        \"train_pct\": 0.5,\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0.1, 1.0),\n",
    "        \"K\": trial.suggest_int(\"K\", 25, 35),\n",
    "        \"layers\": trial.suggest_int(\"layers\", 1, 3),\n",
    "        \"hidden_channels\": trial.suggest_categorical(\"hidden_channels\", [64, 128, 256]),\n",
    "        \"out_channels\": trial.suggest_categorical(\"out_channels\", [64, 128, 256]),\n",
    "        \"norm\": trial.suggest_categorical(\"norm\", [None, \"layernorm\", \"graphnorm\"]),\n",
    "        \"dropout\": trial.suggest_float(\"dropout\", 0.1, 0.5),\n",
    "        \"margin\": trial.suggest_float(\"margin\", 0.1, 1.0),\n",
    "        \"lpl_weight\": trial.suggest_float(\"lpl_weight\", 0.1, 1.0),\n",
    "        \"ratio\": trial.suggest_float(\"ratio\", 0.1, 0.5),\n",
    "        \"pos_weight\": trial.suggest_float(\"pos_weight\", 0.5, 2.0),\n",
    "        \"aggregation\": trial.suggest_categorical(\"aggregation\", [\"sum\", \"mean\", \"max\"]),\n",
    "        \"treatment\": trial.suggest_categorical(\"treatment\", [\"removal\", \"relabeling\"]),\n",
    "        \"seeds\": 5,\n",
    "        \"output_csv\": \"pubmed_sar.csv\"\n",
    "    }\n",
    "    \n",
    "    # Call the experiment function with these parameters.\n",
    "    # run_nnif_gnn_experiment returns (avg_f1, std_f1)\n",
    "    avg_f1, std_f1 = run_nnif_gnn_experiment(params)\n",
    "    \n",
    "    # We aim to maximize F1 score.\n",
    "    return avg_f1\n",
    "\n",
    "# Create an Optuna study to maximize the F1 score.\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print out the best hyperparameters and corresponding F1 score.\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Average F1:\", trial.value)\n",
    "print(\"  Best parameters:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization WikiCS\n",
    "#### SCAR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .train_NNIF_GNN import run_nnif_gnn_experiment\n",
    "import optuna\n",
    "from typing import Dict, Any\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Objective function for hyperparameter optimization using Optuna.\n",
    "    It builds a parameter dictionary, calls the experiment function, and\n",
    "    returns the average F1 score.\n",
    "    \"\"\"\n",
    "    params: Dict[str, Any] = {\n",
    "        \"dataset_name\": \"wiki-cs\",      \n",
    "        \"mechanism\": \"SCAR\",\n",
    "        \"train_pct\": 0.5,\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0.1, 1.0),\n",
    "        \"K\": trial.suggest_int(\"K\", 25, 35),\n",
    "        \"layers\": trial.suggest_int(\"layers\", 1, 3),\n",
    "        \"hidden_channels\": trial.suggest_categorical(\"hidden_channels\", [64, 128, 256]),\n",
    "        \"out_channels\": trial.suggest_categorical(\"out_channels\", [64, 128, 256]),\n",
    "        \"norm\": trial.suggest_categorical(\"norm\", [None, \"layernorm\", \"graphnorm\"]),\n",
    "        \"dropout\": trial.suggest_float(\"dropout\", 0.1, 0.5),\n",
    "        \"margin\": trial.suggest_float(\"margin\", 0.1, 1.0),\n",
    "        \"lpl_weight\": trial.suggest_float(\"lpl_weight\", 0.1, 1.0),\n",
    "        \"ratio\": trial.suggest_float(\"ratio\", 0.1, 0.5),\n",
    "        \"pos_weight\": trial.suggest_float(\"pos_weight\", 0.5, 2.0),\n",
    "        \"aggregation\": trial.suggest_categorical(\"aggregation\", [\"sum\", \"mean\", \"max\"]),\n",
    "        \"treatment\": trial.suggest_categorical(\"treatment\", [\"removal\", \"relabeling\"]),\n",
    "        \"seeds\": 5,\n",
    "        \"output_csv\": \"wikics_sar.csv\"\n",
    "    }\n",
    "    \n",
    "    # Call the experiment function with these parameters.\n",
    "    # run_nnif_gnn_experiment returns (avg_f1, std_f1)\n",
    "    avg_f1, std_f1 = run_nnif_gnn_experiment(params)\n",
    "    \n",
    "    # We aim to maximize F1 score.\n",
    "    return avg_f1\n",
    "\n",
    "# Create an Optuna study to maximize the F1 score.\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print out the best hyperparameters and corresponding F1 score.\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Average F1:\", trial.value)\n",
    "print(\"  Best parameters:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization WikiCS\n",
    "#### SAR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .train_NNIF_GNN import run_nnif_gnn_experiment\n",
    "import optuna\n",
    "from typing import Dict, Any\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Objective function for hyperparameter optimization using Optuna.\n",
    "    It builds a parameter dictionary, calls the experiment function, and\n",
    "    returns the average F1 score.\n",
    "    \"\"\"\n",
    "    params: Dict[str, Any] = {\n",
    "        \"dataset_name\": \"wiki-cs\",      \n",
    "        \"mechanism\": \"SAR\",\n",
    "        \"train_pct\": 0.5,\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0.1, 1.0),\n",
    "        \"K\": trial.suggest_int(\"K\", 25, 35),\n",
    "        \"layers\": trial.suggest_int(\"layers\", 1, 3),\n",
    "        \"hidden_channels\": trial.suggest_categorical(\"hidden_channels\", [64, 128, 256]),\n",
    "        \"out_channels\": trial.suggest_categorical(\"out_channels\", [64, 128, 256]),\n",
    "        \"norm\": trial.suggest_categorical(\"norm\", [None, \"layernorm\", \"graphnorm\"]),\n",
    "        \"dropout\": trial.suggest_float(\"dropout\", 0.1, 0.5),\n",
    "        \"margin\": trial.suggest_float(\"margin\", 0.1, 1.0),\n",
    "        \"lpl_weight\": trial.suggest_float(\"lpl_weight\", 0.1, 1.0),\n",
    "        \"ratio\": trial.suggest_float(\"ratio\", 0.1, 0.5),\n",
    "        \"pos_weight\": trial.suggest_float(\"pos_weight\", 0.5, 2.0),\n",
    "        \"aggregation\": trial.suggest_categorical(\"aggregation\", [\"sum\", \"mean\", \"max\"]),\n",
    "        \"treatment\": trial.suggest_categorical(\"treatment\", [\"removal\", \"relabeling\"]),\n",
    "        \"seeds\": 5,\n",
    "        \"output_csv\": \"wikics_sar.csv\"\n",
    "    }\n",
    "    \n",
    "    # Call the experiment function with these parameters.\n",
    "    # run_nnif_gnn_experiment returns (avg_f1, std_f1)\n",
    "    avg_f1, std_f1 = run_nnif_gnn_experiment(params)\n",
    "    \n",
    "    # We aim to maximize F1 score.\n",
    "    return avg_f1\n",
    "\n",
    "# Create an Optuna study to maximize the F1 score.\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print out the best hyperparameters and corresponding F1 score.\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Average F1:\", trial.value)\n",
    "print(\"  Best parameters:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
